fl <- dir_ls(path = dir,
glob = "*.mdb")
## Check for already processed campaigns
if(file_exists(paste0(dir,comp_file))){
camp_comp <- fread(paste0(dir,comp_file)) %>%
pull(.data$campaign) %>%
unique()
p <- str_split(fl[1], "/")[[1]]
p <- paste0(p[1:length(p)-1], collapse = "/")
camp_comp <- paste0(p,"/",camp_comp,".mdb")
}else{
camp_comp <- NULL
}
# Get only campaigns that haven't yet been compiled
fl <- subset(fl,
!(fl %in% camp_comp))
if(length(fl) == 0){par.ls <- NULL; cat("No new deployments available")}else{
# par.ls <- split(fl, cut(seq_along(fl), floor(length(fl)/5), labels = FALSE))
par.ls <- fl
}
dataDive <- seaTracks:::combDive(par.ls, parallel = parallel)
## Check for already processed campaigns
if(file_exists(paste0(acc_path,comp_file))){
camp_comp <- fread(paste0(dir,comp_file)) %>%
pull(.data$campaign) %>%
unique()
p <- str_split(fl[1], "/")[[1]]
p <- paste0(p[1:length(p)-1], collapse = "/")
camp_comp <- paste0(p,"/",camp_comp,".mdb")
}else{
camp_comp <- NULL
}
# Get only campaigns that haven't yet been compiled
fl <- subset(fl,
!(fl %in% camp_comp))
if(length(fl) == 0){par.ls <- NULL; cat("No new deployments available")}else{
# par.ls <- split(fl, cut(seq_along(fl), floor(length(fl)/5), labels = FALSE))
par.ls <- fl
}
# compile_dive <- function(acc_path  = "./access_files",
#                          dir = "./compiled_raw_datasets",
#                          comp_file = "dive_all_raw_pre-qc",
#                          parallel = FALSE){
acc_path  = "./access_files"
dir = "./compiled_raw_datasets"
comp_file = "dive_all_raw_pre-qc"
parallel = FALSE
dir_create(path = dir) # Create directory for the compiled loc, dive, ctd and haulout datasets
comp_file <- paste0("/",comp_file,".txt")
dir_create(path = dir) # Create directory for the compiled loc, dive, ctd and haulout datasets
comp_file <- paste0("/",comp_file,".txt")
## Get list of all ACCESS files
fl <- dir_ls(path = acc_path,
glob = "*.mdb")
## Check for already processed campaigns
if(file_exists(paste0(dir,comp_file))){
camp_comp <- fread(paste0(dir,comp_file)) %>%
pull(.data$campaign) %>%
unique()
p <- str_split(fl[1], "/")[[1]]
p <- paste0(p[1:length(p)-1], collapse = "/")
camp_comp <- paste0(p,"/",camp_comp,".mdb")
}else{
camp_comp <- NULL
}
# Get only campaigns that haven't yet been compiled
fl <- subset(fl,
!(fl %in% camp_comp))
if(length(fl) == 0){par.ls <- NULL; cat("No new deployments available")}else{
# par.ls <- split(fl, cut(seq_along(fl), floor(length(fl)/5), labels = FALSE))
par.ls <- fl
}
dataDive <- seaTracks:::combDive(par.ls, parallel = parallel)
install.packages("pbapply")
dataDive <- seaTracks:::combDive(par.ls, parallel = parallel)
x=par.ls[1]
datDive = lapply(x, function(z){
openDive(z)
}) %>%
bind_rows() %>%
rowwise() %>%
mutate(campaign = str_split(ref, pattern = "[\\_-]+")[[1]][1],
de_date = as.POSIXct(de_date, format = "%Y-%m-%d %H:%M:%S", tz = "UTC"))
openDive <- function(x){
fl_split = str_split(x, pattern = "/")[[1]]
theDB = fl_split[length(fl_split)] %>%
str_remove(pattern = ".mdb")
thePath = paste0(fl_split[1:(length(fl_split)-1)],
collapse = "/")
cat("Processing dives: ", theDB, "\n")
data = tryCatch( # Catch any errors coming from there being no dive data
get.SRDLdb(theDB = theDB,
theTable = "dive",
theFields = "All",
theRef = "All",
thePath = thePath
),
error = function(e){
message(paste("Unable to find dive data for", str_split(x, "/")[[1]][length(str_split(x, "/")[[1]])]))
})
if(is.null(data)){
# odbcClose(con) # close ACCESS db connection
return(data.frame(REF = str_split(x, "/")[[1]][length(str_split(x, "/")[[1]])]))
}else{
data = data %>%
qcDive(flag.only = TRUE,
max.v.speed = 5) %>%
mutate(
across( # Coercing everything to character prior to binding
.cols = everything(),
.fns = as.character
)
) %>%
rename_with(~ str_replace_all(.x, pattern = "\\.", replacement = "_")) %>%
rename_with(.cols = everything(), .fn = tolower) # Change all colnames to lowercase
return(data)
}
}
datDive = lapply(x, function(z){
openDive(z)
}) %>%
bind_rows() %>%
rowwise() %>%
mutate(campaign = str_split(ref, pattern = "[\\_-]+")[[1]][1],
de_date = as.POSIXct(de_date, format = "%Y-%m-%d %H:%M:%S", tz = "UTC"))
x= par.ls[2]
datDive = lapply(x, function(z){
openDive(z)
}) %>%
bind_rows() %>%
rowwise() %>%
mutate(campaign = str_split(ref, pattern = "[\\_-]+")[[1]][1],
de_date = as.POSIXct(de_date, format = "%Y-%m-%d %H:%M:%S", tz = "UTC"))
z=x
library(seaTracks)
campaigns <- c("ct164", "ct79", "ct91")
providers <- c("imos", "imos", "imos")
get_SMRU_files(campaigns = campaigns, providers = providers)
?dbGetQuery
use_import_from("DBI", c("dbConnect", "dbListTables", "dbDisconnect"))
load_all()
remove.packages("seaTracks")
install()
library(seaTracks)
compile_dive()
?fit_mpm
remove.packages("seaTracks")
load_all()
process_tracks()
compile_diag()
compile_ctd()
process_tracks()
remDat1
exists(remDat1)
?exists
exists("remDat1")
load_all
load_all()
process_tracks()
load_all()
prep_diag
load_all()
process_tracks()
process_tracks
?process_tracks
load_all()
process_tracks()
process_tracks
in_loc = "./compiled_raw_datasets/loc_all_raw_pre-qc.txt"
out_dir = "./processed_datasets/"
out_loc = "loc_ssm_6h"
in_dive = "./compiled_raw_datasets/dive_all_raw_pre-qc.txt"
out_dive = "loc_ssm_dive"
in_ctd = "./compiled_raw_datasets/ctd_all_raw_pre-qc.txt"
out_ctd = "loc_ssm_ctd"
dep_date = NULL # Metadata dataframe with animal id and deployment date
append = TRUE # would you like the dataset to be appended to a previously processed dataset, or processed from scratch?
note_discards = TRUE # would you like a running tally of ids that have been discarded over the course of processing?
add_mpm = TRUE # should a move persistence model be fitted in addition to the ssm?
mpm_model = "mpm" # either c("mpm" or "jmpm") see aniMotum::fit_mpm for details
min_d = 5 # minimum number of days for a deployment to be processed
tstep = 6 # regular timestep for ssm to be fitted to (units = hours)
tstep_units = "hours" # can be one of following c("hour", "hours", "min", "mins", "sec", "secs")
vmax = 4 # m/s
max_tgap = 4 # flags a timestep gap >= "max_tgap" in data (units  = days)
parallel = TRUE # should this run in parallel?
chunk_prop = 0.1 # chunk size relative to the full datablock
return_output = FALSE
dir_create(path = out_dir)
if(!is.null(dep_date)){
dep_date <- fread(dep_date)
}
# Get list of animal ids and split into proportional chunks based on "chunk_prop"
ref_ls <- fread(in_loc, select = "ref") %>%
pull(ref) %>%
unique() %>%
na.omit()
chunk_size <- floor(length(ref_ls)*chunk_prop)
ref_ls <- split(ref_ls, ceiling(seq_along(ref_ls)/chunk_size))
ii=1
n_chunk <- length(ref_ls)
ref_sub <- ref_ls[[ii]]
print(paste0("preparing data from chunk: [", ii,"] of [", n_chunk, "]"))
diag <- prep_diag(in_loc, out_dir, out_loc, dep_date, append, min_d, ref_sub)
d1 <- diag[[1]]
if(nrow(d1) == 0){
like_reason <- ifelse(is.na(diag[[2]]$reason[1]), "already processed", diag[[2]]$reason[1])
print(paste0("skipping data chunk [",ii,"] as no location data available. Likely reason: ", like_reason))
next
}
remDat <- diag[[2]]
if(!is.null(max_tgap)){
# tgaps <- d1 %>%
#   group_by(id) %>%
#   mutate(tgap = difftime(time1 = lead(date),
#                          time2 = date,
#                          units = "days"
#                         ),
#          n.date = lead(date)) %>%
#   filter(tgap > 4) %>%
#   dplyr::select(c(id, date, n.date))
tgaps <- d1 %>%
group_by(id) %>%
mutate(tgap = interval(date, lead(date)),
tgap_dur = tgap/ddays(1)) %>%
filter(tgap_dur > max_tgap) %>%
select(id, tgap)
}
# Load dive data if provided
if(!is.null(in_dive)){
dive <- prep_dive(in_dive, out_dir, out_dive, dep_date, append, ref_sub)
dive <- dive[[1]]
}
# Load ctd data if provided
if(!is.null(in_ctd)){
ctd <- prep_ctd(in_ctd, out_dir, out_ctd, dep_date, append, ref_sub)
ctd <- ctd[[1]]
}
by_step <- paste0(tstep," ",tstep_units)
if(!is.null(tstep) & nrow(d1) > 0){
diag_tstep <- d1 %>%
group_by(id) %>%
summarise(date1 = ceiling_date(min(date, na.rm = T),
unit = tstep_units),
date2 = floor_date(max(date, na.rm = T),
unit = tstep_units),
date = paste(format(seq(date1, date2, by=by_step), '%Y-%m-%d %H:%M:%S'), collapse = ",")
) %>%
select(id, date) %>%
separate_rows(date, sep = ",") %>%
mutate(date = as.POSIXct(date, tz = "UTC"))
}else{
diag_tstep <- d1 %>%
select(id, date)}
pred_times <- diag_tstep
if(!is.null(in_dive)){
pred_times <- pred_times %>%
bind_rows(dive %>% select(id, date))
}
if(!is.null(in_ctd)){
pred_times <- pred_times %>%
bind_rows(ctd %>% select(id, date))
}
pred_times <- pred_times %>%
group_by(id) %>%
mutate(date = as.POSIXct(date, tz = "UTC")) %>%
arrange(date, .by_group = TRUE) %>%
distinct
## Step 2. Fit the SSM (and mpm if selected)
d3 <- data.frame(d1)
par.ls <- unique(d3$id)
print(paste0("Starting to run SSMs for data chunk: [", ii,"] of [", n_chunk, "] | This will take a while if you have many tracks. Sit tight..."))
if(parallel){
plan("future::multisession")
ts <- Sys.time()
dat_size <- floor(length(par.ls)*chunk_prop)
fit_all <- future_lapply(split(par.ls, ceiling(seq_along(par.ls)/dat_size)), function(z) {
# fit_all <- future_lapply(par.ls, function(z) {
## time step of 24 = 1 per day
print(paste0("processing track IDs: ", paste0(z, collapse = ", ")))
fits <- aniMotum::fit_ssm(x = subset(d3, id %in% z),
vmax=vmax,
model="crw",
control = ssm_control(optim=c("nlminb")),
time.step = subset(pred_times, id %in% z)
)
fit_dat <- grab(fits, "predicted", as_sf=FALSE)
if(add_mpm){ # only adds move persistence model for regular timestep and not for individual dives and ctd profiles
sub_mpm <- fit_dat %>%
right_join(diag_tstep) %>%
drop_na(date)
# fits <- fit_mpm(fits, what = "predicted", model = mpm_model)
fits <- fit_mpm(sub_mpm, what = "predicted", model = mpm_model)
mpm_dat <- grab(fits, "fitted", as_sf=FALSE)
if(nrow(mpm_dat) > 0){
suppressMessages(
fit_dat <- left_join(fit_dat, mpm_dat)
)
}
}
return(fit_dat)
})
difftime(Sys.time(),ts)
plan(sequential)
}else{
fits <- aniMotum::fit_ssm(d3, vmax=vmax, model="crw",
control = ssm_control(optim=c("nlminb")),
time.step = pred_times)
fit_dat <- grab(fits, "predicted", as_sf=FALSE)
if(add_mpm){
sub_mpm <- fit_dat %>%
right_join(diag_tstep) %>%
drop_na(date)
# fits <- fit_mpm(fits, what = "predicted", model = mpm_model)
fits <- fit_mpm(sub_mpm, what = "predicted", model = mpm_model)
mpm_dat <- grab(fits, "fitted", as_sf=FALSE)
suppressMessages(
fit_dat <- left_join(fit_dat, mpm_dat)
)
}
fit_all <- fit_dat
}
fits <- aniMotum::fit_ssm(d3, vmax=vmax, model="crw",
control = ssm_control(optim=c("nlminb")),
time.step = pred_times)
View(d3)
d3 %>% nrow()
d3 %>% drop_na(date) %>%  nrow()
d3 %>% na.omit() %>%  nrow()
pred_times %>% nrow()
pred_times %>% drop_na(date) %>%  nrow()
by_step <- paste0(tstep," ",tstep_units)
if(!is.null(tstep) & nrow(d1) > 0){
diag_tstep <- d1 %>%
group_by(id) %>%
summarise(date1 = ceiling_date(min(date, na.rm = T),
unit = tstep_units),
date2 = floor_date(max(date, na.rm = T),
unit = tstep_units),
date = paste(format(seq(date1, date2, by=by_step), '%Y-%m-%d %H:%M:%S'), collapse = ",")
) %>%
select(id, date) %>%
separate_rows(date, sep = ",") %>%
mutate(date = as.POSIXct(date, tz = "UTC")) %>%
drop_na()
}else{
diag_tstep <- d1 %>%
select(id, date) %>%
drop_na()}
pred_times <- diag_tstep
if(!is.null(in_dive)){
pred_times <- pred_times %>%
bind_rows(dive %>% select(id, date))
}
if(!is.null(in_ctd)){
pred_times <- pred_times %>%
bind_rows(ctd %>% select(id, date))
}
pred_times <- pred_times %>%
group_by(id) %>%
mutate(date = as.POSIXct(date, tz = "UTC")) %>%
arrange(date, .by_group = TRUE) %>%
distinct %>%
drop_na()
## Step 2. Fit the SSM (and mpm if selected)
d3 <- data.frame(d1)
par.ls <- unique(d3$id)
print(paste0("Starting to run SSMs for data chunk: [", ii,"] of [", n_chunk, "] | This will take a while if you have many tracks. Sit tight..."))
fits <- aniMotum::fit_ssm(d3, vmax=vmax, model="crw",
control = ssm_control(optim=c("nlminb")),
time.step = pred_times)
fit_dat <- grab(fits, "predicted", as_sf=FALSE)
if(add_mpm){
sub_mpm <- fit_dat %>%
right_join(diag_tstep) %>%
drop_na(date)
# fits <- fit_mpm(fits, what = "predicted", model = mpm_model)
fits <- fit_mpm(sub_mpm, what = "predicted", model = mpm_model)
mpm_dat <- grab(fits, "fitted", as_sf=FALSE)
suppressMessages(
fit_dat <- left_join(fit_dat, mpm_dat)
)
}
View(fit_dat)
fit_dat_sort <- fit_dat %>% arrange(id, date)
View(fit_dat_sort)
?arrange
fit_dat_sort <- fit_dat %>% group_by(id) %>% arrange(asc(date), .by_group = TRUE)
fit_dat_sort <- fit_dat %>% group_by(id) %>% arrange(date, .by_group = TRUE)
load_all()
process_tracks()
process_tracks()
load_all()
process_tracks()
use_package("sf", "Suggests")
use_package("sf", "Suggests", min_version = TRUE)
load_all()
process_tracks(chunk_prop = 1)
use_readme_md(open = rlang::is_interactive())
library(devtools);library(roxygen2)
library(fs)
install.packages("devtools")
renv::status()
?renv::status()
renv::restore()
library(devtools)
library(roxygen2)
install()
library(roxygen2);library(usethis)
install
install()
library(devtools)
install()
check()
devtools::document()
warnings()
?use_import_from
devtools::document()
load_all()
warnings()
load_all()
warnings()
load_all()
devtools::document()
check()
install()
check()
use_import_from("dplyr", "right_join")
use_import_from("tidyr", "drop_na")
load
load()
load_all()
check()
install()
?dplyr::semi_join
# install.packages("devtools")
devtools::install_github("davo-b-green/seaTracks")
as.POSIXct(NA)
class(as.POSIXct(NA))
load_all()
check()
load_all()
install()
check()
chunk_prop = 0.1
dat_rows <- 1:100
chunk_size <- floor(length(dat_rows)*chunk_prop)
dat_rows <- split(dat_rows, ceiling(seq_along(dat_rows)/chunk_size))
dat_rows
?filter
in_dive = "./processed_datasets/loc_ssm_dive.csv"
## Bring in dive dataset
dive <- fread(in_dive)
library(tidyverse)
library(data.table)
library(nlme)
library(purrr)
library(future.apply)
## Bring in dive dataset
dive <- fread(in_dive)
in_dive <- "../../seaTracks/processed_datasets/loc_ssm_dive.csv"
## Bring in dive dataset
dive <- fread(in_dive)
filter_flag = TRUE
## Creating general dive metrics
dive.met <- dive %>%
select(id, date, depth_str, propn_str, dive_dur, max_dep, surf_dur, d_mask) %>%
if(filter_flag){
filter(d_mask == 0)
} %>% # Filter using Biuw qc - removing dives with vertical rates > 5m/s
filter(max_dep < 1200) %>% ## Remove dives following Cox et al. as laid out in Allegue et al. 2023
filter(max_dep > 15) %>%
filter(dive_dur < 2800) %>%
filter(dive_dur > 300) %>%
filter(surf_dur < 300)
## Creating general dive metrics
dive.met <- dive %>%
select(id, date, depth_str, propn_str, dive_dur, max_dep, surf_dur, d_mask) %>%
filter(d_mask == 0) %>% # Filter using Biuw qc - removing dives with vertical rates > 5m/s
filter(max_dep < 1200) %>% ## Remove dives following Cox et al. as laid out in Allegue et al. 2023
filter(max_dep > 15) %>%
filter(dive_dur < 2800) %>%
filter(dive_dur > 300) %>%
filter(surf_dur < 300)
dat <- data.frame(yday = 1:366)
dat$yday[dat$yday>300] <- -c(366-dat$yday[dat$yday>300])
dat
library(NCmisc)
trial_fn_ls <- list.functions.in.file("./R/residuals_metrics_calc.R")
lapply(names(trial_fn_ls), function(x){use_import_from(str_remove(x, "package:"), trial_fn_ls[[x]])})
trial_fn_ls$`package:stats`
library(usethis)
?lme
??lme
use_import_from("nlme", c("lme", "lmeControl"))
use_import_from("nlme", c("predict"))
library(devtools)
load_all()
load_all()
check()
??na.exclude
use_import_from("stats", c("fitted", "na.exclude", "predict", "quantile",
"resid", "runif", "time"))
use_import_from("dplyr", "bind_cols")
load_all()
check()
use_import_from("dplyr", "ungroup")
load_all()
check()
load_all()
check()
check()
